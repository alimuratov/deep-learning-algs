{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "181f73a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6821cd1",
   "metadata": {},
   "source": [
    "The following function initializes parameters for a **2-layer Neural Network**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0321366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(nx, nh, ny):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    nx --- size of the input layer\n",
    "    nh --- size of the hidden layer\n",
    "    ny --- size of the output layer\n",
    "    By \"size\" we mean the number of units in a particular layer\n",
    "    \n",
    "    Outputs:\n",
    "    parameters --- python dictionary containing initialized parameters:\n",
    "        W1 - weight matrix of shape (nh, nx)\n",
    "        b1 - bias vector of shape (nh, 1)\n",
    "        W2 - weight matrix of shape (ny, nh)\n",
    "        b2 - bias vector of shape (ny, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1) # to make the results of the function call reproducible\n",
    "    \n",
    "    # initializing weight parameters with random values\n",
    "    # and bias parameters with zeros\n",
    "    W1 = np.random.randn(nh, nx) * 0.01\n",
    "    b1 = np.zeros((nh, 1))\n",
    "    W2 = np.random.randn(ny, nh) * 0.01\n",
    "    b2 = np.zeros((ny, 1))\n",
    "    # we multiply matrices of weight parameters with 0.01 to avoid \"neuron saturation\"\n",
    "    # if weights are too large, neurons can become saturated\n",
    "    # this means that they become less sensitive to changes in the input\n",
    "    # slowing down learning during training\n",
    "    \n",
    "    # storing the initialized parameters in a dictionary\n",
    "    # to facilitate convenient retrieval at a later time\n",
    "    parameters = {\n",
    "        \"W1\": W1,\n",
    "        \"b1\": b1,\n",
    "        \"W2\": W2,\n",
    "        \"b2\": b2\n",
    "    }\n",
    "    \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04cbc183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(3,2,1)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab71ade7",
   "metadata": {},
   "source": [
    "The following function initializes paramateres for an **L-layer Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c95233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layers):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    layers - list containing the number of units for each layer in the network\n",
    "    \n",
    "    Outputs:\n",
    "    parameters - dictionary containing parameters for every layer in the network\n",
    "        Wl - weight matrix of shape (layers[l], layers[l-1])\n",
    "        b1 - bias column vector of shape (slayers[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layers)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers[l], layers[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layers[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layers[l], layers[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layers[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e01a35c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cd7299",
   "metadata": {},
   "source": [
    "The following function performs **linear part of the forward propagation step** by computing the linear combination of input features (activations of the previous) layer and corresponding weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3aa003c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        A - matrix of activations from the previous layer\n",
    "            (size of the previous layer, number of training examples)\n",
    "        W - weights matrix\n",
    "            (size of the current layer, size of previous layer)\n",
    "        b - bias vector\n",
    "            (size of the current layer, 1)\n",
    "            \n",
    "    Outputs:\n",
    "        Z - pre-activation parameter (linear combination of weights and input features)\n",
    "            (size of the current layer, number of trainig examples)\n",
    "        cache - tuple containing \"A\", \"W\" and \"b\"; storeing values for efficient computation of the backward pass\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617d557a",
   "metadata": {},
   "source": [
    "The following function combines both Linear and Activation functions. It accepts activations from the previous layer and parameters specific to the current layer to compute the linear combination component of the forward propagation step. Subsequently, it invokes the activation function of the current layer to determine the activation for that layer. To optimize the efficiency of the backward pass, the computed values within the function are stored in a cache. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4662264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        A_prev - activations from the previous layer\n",
    "            (size of the previous layer, number of training examples)\n",
    "        W - weights matrix\n",
    "            (size of the current layer, size of the previous layer)\n",
    "        b - bias vector \n",
    "            (size of the current layer, 1)\n",
    "        activation - the name of the activation function of the current layer\n",
    "        \n",
    "    Outputs:\n",
    "        A - post-activation value\n",
    "            (the size of the current layer, number of training examples)\n",
    "        cache - tuple containing linear_cache and activation_cache\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdbdd4f",
   "metadata": {},
   "source": [
    "The function performs the forward propagation step from the initial (input) layer to the final layer in a Neural Network. It utilizes a for loop to iterate through each layer of the network. At each iteration, the function applies the activation function specific to the current layer to compute post-activation values and cache. The activation function used for the first `L-1` layers is ReLU, while the last layer employs the Sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee046443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    X - input features (activations of the layer zero)\n",
    "        (size of the layer zero, number of training examples)\n",
    "    parameters - output of initialize_parameters_deep(), contains \n",
    "                 parameter values for each layer \n",
    "    \n",
    "    Outputs:\n",
    "    AL - post-activation value from the output layer\n",
    "    caches - list of caches from linear_activation_forward()\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # number of layers in NN\n",
    "    # dictionary \"parameters\" has 2 parameter values (weights and bias term) for each layer\n",
    "    # so diving its length by 2 gives the number of layers in NN\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(l)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ae6405",
   "metadata": {},
   "source": [
    "This function computes the cost to quantify the error between true labels and predicted labels. By calculating the derivatives of the cost function with respect to the parameters, we can determine the magnitude and direction of the required parameter changes. These changes are then used to update the model, enabling it to generate more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e877472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    AL - the post-activation value of the output layer\n",
    "        (1, number of training examples)\n",
    "    Y - the vector containing true labels for the training examples\n",
    "        (1, number of training examples)\n",
    "        \n",
    "    Outputs:\n",
    "    cost - cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = -(np.dot(Y, np.log(AL).T) + np.dot(1-Y, np.log(1-AL).T)) / m\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59576597",
   "metadata": {},
   "source": [
    "The next function implements the linear part of backward propagation for a single layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "883a052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    dZ - gradient of the cost with respect to the current layer's linear output\n",
    "        (size of the current layer, number of training examples)\n",
    "    cache - cached values (A_prev, W, b) from the forward pass\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev - gradient of the cost with respect to the activation of the previous layer\n",
    "        (size of the previous layer, number of training examples)\n",
    "    dW - gradient of the cost with respect to weights of the current layer\n",
    "        (size of the current layer, size of the previous layer)\n",
    "    db - gradient of the cost with respect to the bias of the current layer\n",
    "        (size of the current layer, 1)\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis = 1, keepdims = True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return  dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c10892",
   "metadata": {},
   "source": [
    "$$dW_{i,j} = \\frac{1}{m} \\sum_m dZ[i][m] * A_{\\text{prev}}[m][j]$$\n",
    "It is worth mentioning that $dZ[i][m]$ and $A_{\\text{prev}}[m][j]$ represent the same input feature of a given training example. Summing the products of these values across different training examples and subsequently dividing by $m$ ensures that the derivative of the cost with respect to the weight of each input feature takes into account every training example of that feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f79850a",
   "metadata": {},
   "source": [
    "This function combines the operations of `linear_backward()` (dZ → dA_prev, dW, db) with the backpropagation steps of the activation functions `sigmoid_backward()` and `relu_backward()` (dA → dZ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0cac2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    dA - post-activation gradient\n",
    "        (size of the current layer, m training examples)\n",
    "    cache - tuple of values (linear_cache, activation cache)\n",
    "    activation - the name of activation function used in this layer\n",
    "    \n",
    "    Outputs:\n",
    "    dA_prev - gradient of the cost with respect to the activations of the previous layer\n",
    "    dW, db - gradients of the cost with respect to parameters\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c23656",
   "metadata": {},
   "source": [
    "The following function implements backpropagation for the [LINEAR->RELU]  ×\n",
    "  (L-1) -> LINEAR -> SIGMOID model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f96f3629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    AL - probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y - true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches - list of caches\n",
    "                \n",
    "    Outputs:\n",
    "    grads - A dictionary with the gradients\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) \n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dA_prev_temp, current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842c5e0",
   "metadata": {},
   "source": [
    "The last function's purpose is to update the value of each layer's parameters based on the computed gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37160b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    params -  dictionary containing  parameters \n",
    "    grads -  dictionary containing gradients, output of L_model_backward\n",
    "    \n",
    "    Ouputs:\n",
    "    parameters -  dictionary containing  updated parameters \n",
    "    \"\"\"\n",
    "    parameters = params.copy()\n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "    return parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
